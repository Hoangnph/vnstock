#!/usr/bin/env python3
"""
VN100 Indicators Pipeline

Pipeline ch·ªâ t√≠nh to√°n c√°c ch·ªâ s·ªë ph√¢n t√≠ch cho VN100 t·ª´ ƒë·∫ßu t·ªõi gi·ªù.
Pipeline n√†y s·∫Ω:
1. L·∫•y danh s√°ch VN100
2. T√≠nh to√°n c√°c ch·ªâ s·ªë ph√¢n t√≠ch cho t·ª´ng m√£
3. L∆∞u k·∫øt qu·∫£ v√†o database v·ªõi deduplication
4. B·ªè qua ph·∫ßn ch·∫•m ƒëi·ªÉm v√† ƒë√°nh gi√°

Usage:
  python fastapi/pipeline/vn100_indicators_pipeline.py --batch 5 --start-date 2010-01-01
"""

from __future__ import annotations

import argparse
import asyncio
import time
from datetime import date, datetime, timedelta
from typing import Any, Dict, List, Optional, Set

import sys
import os
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from fastapi.func.vn100_database_loader import VN100DatabaseLoader
from database.api.database import get_database_manager, get_async_session
from database.api.repositories import RepositoryFactory
from analytis.engines.indicator_engine import IndicatorEngine
from analytis.config import AnalysisConfig
from analytis.repositories.indicator_repository import IndicatorRepository
from analytis.repositories.config_repository import ConfigRepository


class VN100IndicatorsPipeline:
    """Pipeline ch·ªâ t√≠nh to√°n ch·ªâ s·ªë ph√¢n t√≠ch cho VN100"""
    
    def __init__(self, batch_size: int = 5, start_date: str = "2010-01-01"):
        """
        Kh·ªüi t·∫°o pipeline
        
        Args:
            batch_size: K√≠ch th∆∞·ªõc batch cho x·ª≠ l√Ω
            start_date: Ng√†y b·∫Øt ƒë·∫ßu t√≠nh to√°n
        """
        self.batch_size = batch_size
        self.start_date = start_date
        self.end_date = date.today()
        self.indicator_engine = IndicatorEngine()
        self.processed_symbols: Set[str] = set()
        self.skipped_symbols: Set[str] = set()
        
    async def run_pipeline(self) -> Dict[str, Any]:
        """
        Ch·∫°y pipeline t√≠nh to√°n ch·ªâ s·ªë
        
        Returns:
            Dict v·ªõi k·∫øt qu·∫£ pipeline
        """
        start_time = time.time()
        
        print(f"üöÄ B·∫Øt ƒë·∫ßu VN100 Indicators Pipeline")
        print(f"üìä Th·ªùi gian: {self.start_date} ƒë·∫øn {self.end_date}")
        print(f"üì¶ Batch size: {self.batch_size}")
        
        # B∆∞·ªõc 1: L·∫•y danh s√°ch VN100
        print(f"\n{'='*60}")
        print(f"B∆Ø·ªöC 1: L·∫§Y DANH S√ÅCH VN100")
        print(f"{'='*60}")
        
        symbols = await self._get_vn100_symbols()
        print(f"üìä T√¨m th·∫•y {len(symbols)} m√£ VN100")
        
        # B∆∞·ªõc 2: T√≠nh to√°n ch·ªâ s·ªë ph√¢n t√≠ch
        print(f"\n{'='*60}")
        print(f"B∆Ø·ªöC 2: T√çNH TO√ÅN CH·ªà S·ªê PH√ÇN T√çCH")
        print(f"{'='*60}")
        
        result = await self._calculate_indicators_for_symbols(symbols)
        
        # T·ªïng k·∫øt
        end_time = time.time()
        duration = end_time - start_time
        
        summary = {
            'pipeline_duration_seconds': duration,
            'start_date': self.start_date,
            'end_date': self.end_date.isoformat(),
            'total_symbols': len(symbols),
            'processed_symbols': len(self.processed_symbols),
            'skipped_symbols': len(self.skipped_symbols),
            'success_rate': (len(self.processed_symbols) / len(symbols)) * 100 if symbols else 0,
            'result': result
        }
        
        print(f"\n{'='*60}")
        print(f"T·ªîNG K·∫æT PIPELINE")
        print(f"{'='*60}")
        print(f"‚è±Ô∏è Th·ªùi gian: {duration:.2f} gi√¢y")
        print(f"üìä T·ªïng m√£: {len(symbols)}")
        print(f"‚úÖ ƒê√£ x·ª≠ l√Ω: {len(self.processed_symbols)}")
        print(f"‚è≠Ô∏è B·ªè qua: {len(self.skipped_symbols)}")
        print(f"üìà T·ª∑ l·ªá th√†nh c√¥ng: {summary['success_rate']:.1f}%")
        
        if self.skipped_symbols:
            print(f"‚è≠Ô∏è M√£ b·ªè qua: {', '.join(list(self.skipped_symbols)[:10])}{'...' if len(self.skipped_symbols) > 10 else ''}")
        
        return summary
    
    async def _get_vn100_symbols(self) -> List[str]:
        """L·∫•y danh s√°ch m√£ VN100"""
        get_database_manager().initialize()
        loader = VN100DatabaseLoader()
        
        try:
            symbols = await loader.get_active_vn100_symbols()
            if symbols:
                return symbols
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l·∫•y t·ª´ database: {e}")
        
        # Fallback t·ª´ CSV
        try:
            import pandas as pd
            df = pd.read_csv('assets/data/vn100_official_ssi.csv')
            symbols = df['symbol'].astype(str).str.upper().tolist()
            print(f"üìã S·ª≠ d·ª•ng danh s√°ch t·ª´ CSV: {len(symbols)} m√£")
            return symbols
        except Exception as e:
            print(f"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc CSV: {e}")
            return []
    
    async def _calculate_indicators_for_symbols(self, symbols: List[str]) -> Dict[str, Any]:
        """T√≠nh to√°n ch·ªâ s·ªë cho danh s√°ch m√£"""
        print(f"üìà B·∫Øt ƒë·∫ßu t√≠nh to√°n ch·ªâ s·ªë cho {len(symbols)} m√£")
        
        # L·∫•y ho·∫∑c t·∫°o indicator config
        config_id = await self._get_or_create_indicator_config()
        
        async with get_async_session() as session:
            indicator_repo = IndicatorRepository(session)
            total_batches = (len(symbols) + self.batch_size - 1) // self.batch_size
            
            results = []
            total_calculations = 0
            total_skipped = 0
            
            for i in range(0, len(symbols), self.batch_size):
                batch_num = i // self.batch_size + 1
                chunk = symbols[i:i+self.batch_size]
                
                print(f"üîÑ Batch {batch_num}/{total_batches}: {chunk}")
                
                batch_results = []
                for j, symbol in enumerate(chunk):
                    try:
                        result = await self._calculate_indicators_for_symbol(
                            session, indicator_repo, symbol, config_id
                        )
                        batch_results.append(result)
                        
                        if result['success']:
                            self.processed_symbols.add(symbol)
                            total_calculations += result['calculations_count']
                            print(f"  ‚úÖ {symbol}: {result['calculations_count']} t√≠nh to√°n")
                        else:
                            if result.get('skipped', False):
                                self.skipped_symbols.add(symbol)
                                total_skipped += 1
                                print(f"  ‚è≠Ô∏è {symbol}: B·ªè qua (ƒë√£ c√≥)")
                            else:
                                print(f"  ‚ùå {symbol}: {result.get('error', 'Unknown error')}")
                        
                        # Delay nh·ªè gi·ªØa c√°c m√£
                        if j < len(chunk) - 1:
                            await asyncio.sleep(0.5)
                            
                    except Exception as e:
                        print(f"  ‚ùå {symbol}: Exception - {e}")
                        batch_results.append({
                            "symbol": symbol, 
                            "success": False, 
                            "error": str(e)
                        })
                
                results.extend(batch_results)
                
                # Delay gi·ªØa c√°c batch
                if i + self.batch_size < len(symbols):
                    print(f"‚è≥ Ch·ªù 2 gi√¢y tr∆∞·ªõc batch ti·∫øp theo...")
                    await asyncio.sleep(2.0)
            
            return {
                "total_symbols": len(symbols),
                "processed_symbols": len(self.processed_symbols),
                "skipped_symbols": len(self.skipped_symbols),
                "total_calculations": total_calculations,
                "total_skipped": total_skipped,
                "results": results
            }
    
    async def _get_or_create_indicator_config(self) -> int:
        """L·∫•y ho·∫∑c t·∫°o indicator config"""
        async with get_async_session() as session:
            config_repo = ConfigRepository(session)
            
            # T√¨m config m·∫∑c ƒë·ªãnh
            config = await config_repo.get_default_config("indicator")
            if config:
                print(f"üìã S·ª≠ d·ª•ng indicator config ID: {config['id']}")
                return config['id']
            
            # T·∫°o config m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng c√≥
            print("üìã T·∫°o indicator config m·∫∑c ƒë·ªãnh...")
            config_id = await config_repo.create_config(
                name="Default Indicator Config",
                description="C·∫•u h√¨nh ch·ªâ s·ªë m·∫∑c ƒë·ªãnh cho VN100",
                config_type="indicator",
                config_data=AnalysisConfig().to_dict()
            )
            print(f"‚úÖ ƒê√£ t·∫°o indicator config ID: {config_id}")
            return config_id
    
    async def _calculate_indicators_for_symbol(
        self, 
        session, 
        indicator_repo: IndicatorRepository, 
        symbol: str, 
        config_id: int
    ) -> Dict[str, Any]:
        """T√≠nh to√°n ch·ªâ s·ªë cho m·ªôt m√£"""
        try:
            # L·∫•y d·ªØ li·ªáu OHLCV
            from analytis.data.loader import load_ohlcv_daily
            
            try:
                data = await load_ohlcv_daily(symbol, self.start_date, self.end_date.isoformat())
                if data is None or data.empty:
                    return {
                        "symbol": symbol,
                        "success": False,
                        "error": "No data available"
                    }
            except Exception as e:
                return {
                    "symbol": symbol,
                    "success": False,
                    "error": f"Failed to load data: {e}"
                }
            
            # Ki·ªÉm tra xem ƒë√£ c√≥ d·ªØ li·ªáu ch∆∞a ƒë·ªÉ tr√°nh tr√πng l·∫∑p
            existing_calc = await indicator_repo.get_latest_calculation(symbol, config_id)
            if existing_calc:
                existing_date = existing_calc['calculation_date']
                if existing_date >= data.index.max().date():
                    return {
                        "symbol": symbol,
                        "success": True,
                        "skipped": True,
                        "calculations_count": 0,
                        "message": f"Already up to date (last: {existing_date})"
                    }
            
            # T√≠nh to√°n ch·ªâ s·ªë
            indicators_df = self.indicator_engine.calculate_all_indicators(data)
            
            # L∆∞u k·∫øt qu·∫£ v√†o database (batch processing)
            calculations_count = 0
            import pandas as pd
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu batch
            batch_data = []
            for _, row in indicators_df.iterrows():
                try:
                    indicators_dict = {
                        "ma_short": float(row.get("MA9", 0)) if pd.notna(row.get("MA9")) else None,
                        "ma_long": float(row.get("MA50", 0)) if pd.notna(row.get("MA50")) else None,
                        "rsi": float(row.get("RSI", 0)) if pd.notna(row.get("RSI")) else None,
                        "macd": float(row.get("MACD", 0)) if pd.notna(row.get("MACD")) else None,
                        "macd_signal": float(row.get("Signal_Line", 0)) if pd.notna(row.get("Signal_Line")) else None,
                        "macd_histogram": float(row.get("MACD_Hist", 0)) if pd.notna(row.get("MACD_Hist")) else None,
                        "bb_upper": float(row.get("BB_Upper", 0)) if pd.notna(row.get("BB_Upper")) else None,
                        "bb_middle": float(row.get("MA20", 0)) if pd.notna(row.get("MA20")) else None,
                        "bb_lower": float(row.get("BB_Lower", 0)) if pd.notna(row.get("BB_Lower")) else None,
                        "obv": float(row.get("OBV", 0)) if pd.notna(row.get("OBV")) else None,
                        "ichimoku_conversion": float(row.get("Tenkan_sen", 0)) if pd.notna(row.get("Tenkan_sen")) else None,
                        "ichimoku_base": float(row.get("Kijun_sen", 0)) if pd.notna(row.get("Kijun_sen")) else None,
                        "ichimoku_span_a": float(row.get("Senkou_Span_A", 0)) if pd.notna(row.get("Senkou_Span_A")) else None,
                        "ichimoku_span_b": float(row.get("Senkou_Span_B", 0)) if pd.notna(row.get("Senkou_Span_B")) else None,
                    }
                    
                    batch_data.append({
                        "symbol": symbol,
                        "calculation_date": row.name.date(),
                        "config_id": config_id,
                        "indicators": indicators_dict,
                        "data_points": len(data),
                        "start_date": data.index.min().date(),
                        "end_date": data.index.max().date(),
                        "calculation_duration_ms": 0
                    })
                    
                except Exception as e:
                    print(f"    ‚ö†Ô∏è L·ªói chu·∫©n b·ªã d·ªØ li·ªáu ng√†y {row.name.date()}: {e}")
                    continue
            
            # L∆∞u batch v√†o database
            if batch_data:
                try:
                    await indicator_repo.save_indicator_calculations_batch(batch_data)
                    calculations_count = len(batch_data)
                    print(f"    ‚úÖ ƒê√£ l∆∞u {calculations_count} t√≠nh to√°n ch·ªâ s·ªë")
                except Exception as e:
                    print(f"    ‚ùå L·ªói l∆∞u batch: {e}")
                    # Fallback: l∆∞u t·ª´ng record
                    for data in batch_data:
                        try:
                            await indicator_repo.save_indicator_calculation(
                                symbol=data["symbol"],
                                calculation_date=data["calculation_date"],
                                config_id=data["config_id"],
                                indicators=data["indicators"],
                                data_points=data["data_points"],
                                start_date=data["start_date"],
                                end_date=data["end_date"],
                                calculation_duration_ms=data["calculation_duration_ms"]
                            )
                            calculations_count += 1
                        except Exception as e2:
                            print(f"    ‚ö†Ô∏è L·ªói l∆∞u ng√†y {data['calculation_date']}: {e2}")
                            continue
            
            return {
                "symbol": symbol,
                "success": True,
                "calculations_count": calculations_count,
                "data_points": len(data)
            }
            
        except Exception as e:
            return {
                "symbol": symbol,
                "success": False,
                "error": str(e)
            }


async def main(batch_size: int = 5, start_date: str = "2010-01-01") -> None:
    """H√†m ch√≠nh"""
    pipeline = VN100IndicatorsPipeline(batch_size=batch_size, start_date=start_date)
    
    try:
        result = await pipeline.run_pipeline()
        
        # L∆∞u k·∫øt qu·∫£
        import json
        from datetime import datetime
        
        output_file = f"output/indicators_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nüíæ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_file}")
        
    except Exception as e:
        print(f"‚ùå Pipeline th·∫•t b·∫°i: {e}")
        import traceback
        traceback.print_exc()
        raise


def cli_main() -> int:
    """CLI entry point"""
    parser = argparse.ArgumentParser(description="VN100 Indicators Pipeline")
    parser.add_argument("--batch", dest="batch", type=int, default=5, 
                       help="Batch size for processing (default: 5)")
    parser.add_argument("--start-date", dest="start_date", type=str, default="2010-01-01",
                       help="Start date for calculation (default: 2010-01-01)")
    
    args = parser.parse_args()
    
    asyncio.run(main(args.batch, args.start_date))
    return 0


if __name__ == "__main__":
    raise SystemExit(cli_main())
