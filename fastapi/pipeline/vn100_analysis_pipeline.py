#!/usr/bin/env python3
"""
VN100 Analysis Pipeline

Pipeline t√≠ch h·ª£p c·∫≠p nh·∫≠t d·ªØ li·ªáu VN100 v√† t√≠nh to√°n ch·ªâ s·ªë ph√¢n t√≠ch t·ª± ƒë·ªông.
Pipeline n√†y s·∫Ω:
1. C·∫≠p nh·∫≠t d·ªØ li·ªáu VN100 t·ª´ SSI API
2. T·ª± ƒë·ªông t√≠nh to√°n c√°c ch·ªâ s·ªë ph√¢n t√≠ch cho c√°c m√£ ƒë√£ c·∫≠p nh·∫≠t
3. L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch v√†o database
4. H·ªó tr·ª£ c·∫≠p nh·∫≠t incremental v√† batch processing

Usage:
  python fastapi/pipeline/vn100_analysis_pipeline.py --batch 3 --analysis-days 30
"""

from __future__ import annotations

import argparse
import asyncio
import time
from datetime import date, datetime, timedelta
from typing import Any, Dict, List, Optional, Set

import sys
import os
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from fastapi.func.vn100_database_loader import VN100DatabaseLoader
from fastapi.func.ssi_fetcher_with_tracking import SSIFetcherWithTracking
from database.api.database import get_database_manager, get_async_session
from database.api.repositories import RepositoryFactory
from database.schema.models import DataSource
from analytis.analysis_engine_db import DatabaseIntegratedAnalysisEngine
from analytis.config import AnalysisConfig


class VN100AnalysisPipeline:
    """Pipeline t√≠ch h·ª£p c·∫≠p nh·∫≠t d·ªØ li·ªáu v√† ph√¢n t√≠ch VN100"""
    
    def __init__(self, batch_size: int = 3, analysis_days: int = 30):
        """
        Kh·ªüi t·∫°o pipeline
        
        Args:
            batch_size: K√≠ch th∆∞·ªõc batch cho c·∫≠p nh·∫≠t d·ªØ li·ªáu
            analysis_days: S·ªë ng√†y ph√¢n t√≠ch cho c√°c m√£ ƒë√£ c·∫≠p nh·∫≠t
        """
        self.batch_size = batch_size
        self.analysis_days = analysis_days
        self.data_fetcher = SSIFetcherWithTracking()
        self.analysis_engine = DatabaseIntegratedAnalysisEngine()
        self.updated_symbols: Set[str] = set()
        self.analyzed_symbols: Set[str] = set()
        
    async def run_pipeline(self, target_end_date: Optional[date] = None) -> Dict[str, Any]:
        """
        Ch·∫°y pipeline ho√†n ch·ªânh
        
        Args:
            target_end_date: Ng√†y k·∫øt th√∫c c·∫≠p nh·∫≠t (None = h√¥m nay)
            
        Returns:
            Dict v·ªõi k·∫øt qu·∫£ pipeline
        """
        start_time = time.time()
        
        if target_end_date is None:
            target_end_date = date.today()
        
        print(f"üöÄ B·∫Øt ƒë·∫ßu VN100 Analysis Pipeline")
        print(f"üìä Target end date: {target_end_date}")
        print(f"üì¶ Batch size: {self.batch_size}")
        print(f"üìà Analysis days: {self.analysis_days}")
        
        # B∆∞·ªõc 1: C·∫≠p nh·∫≠t d·ªØ li·ªáu VN100
        print(f"\n{'='*60}")
        print(f"B∆Ø·ªöC 1: C·∫¨P NH·∫¨T D·ªÆ LI·ªÜU VN100")
        print(f"{'='*60}")
        
        data_update_result = await self._update_vn100_data(target_end_date)
        
        # B∆∞·ªõc 2: T√≠nh to√°n ch·ªâ s·ªë ph√¢n t√≠ch
        print(f"\n{'='*60}")
        print(f"B∆Ø·ªöC 2: T√çNH TO√ÅN CH·ªà S·ªê PH√ÇN T√çCH")
        print(f"{'='*60}")
        
        analysis_result = await self._run_analysis_for_updated_symbols()
        
        # T·ªïng k·∫øt
        end_time = time.time()
        duration = end_time - start_time
        
        summary = {
            'pipeline_duration_seconds': duration,
            'target_end_date': target_end_date.isoformat(),
            'data_update': data_update_result,
            'analysis': analysis_result,
            'total_updated_symbols': len(self.updated_symbols),
            'total_analyzed_symbols': len(self.analyzed_symbols),
            'success_rate': {
                'data_update': (data_update_result['success_count'] / data_update_result['total_symbols']) * 100 if data_update_result['total_symbols'] > 0 else 0,
                'analysis': (analysis_result['success_count'] / analysis_result['total_symbols']) * 100 if analysis_result['total_symbols'] > 0 else 0
            }
        }
        
        print(f"\n{'='*60}")
        print(f"T·ªîNG K·∫æT PIPELINE")
        print(f"{'='*60}")
        print(f"‚è±Ô∏è Th·ªùi gian: {duration:.2f} gi√¢y")
        print(f"üìä C·∫≠p nh·∫≠t d·ªØ li·ªáu: {data_update_result['success_count']}/{data_update_result['total_symbols']} m√£")
        print(f"üìà Ph√¢n t√≠ch: {analysis_result['success_count']}/{analysis_result['total_symbols']} m√£")
        print(f"üìã T·ªïng b·∫£n ghi c·∫≠p nh·∫≠t: {data_update_result['total_records']:,}")
        print(f"üéØ T·ªïng t√≠n hi·ªáu ph√¢n t√≠ch: {analysis_result['total_signals']:,}")
        
        if data_update_result['failed_symbols']:
            print(f"‚ùå M√£ c·∫≠p nh·∫≠t th·∫•t b·∫°i: {', '.join(data_update_result['failed_symbols'])}")
        
        if analysis_result['failed_symbols']:
            print(f"‚ùå M√£ ph√¢n t√≠ch th·∫•t b·∫°i: {', '.join(analysis_result['failed_symbols'])}")
        
        return summary
    
    async def _update_vn100_data(self, target_end_date: date) -> Dict[str, Any]:
        """C·∫≠p nh·∫≠t d·ªØ li·ªáu VN100"""
        get_database_manager().initialize()
        loader = VN100DatabaseLoader()
        symbols = await loader.get_active_vn100_symbols()
        
        if not symbols:
            # Fallback t·ª´ CSV
            import pandas as pd
            df = pd.read_csv('assets/data/vn100_official_ssi.csv')
            symbols = df['symbol'].astype(str).str.upper().tolist()
        
        print(f"üìä C·∫≠p nh·∫≠t {len(symbols)} m√£ VN100")
        
        async with get_async_session() as session:
            results: List[Dict[str, Any]] = []
            total_batches = (len(symbols) + self.batch_size - 1) // self.batch_size
            
            for i in range(0, len(symbols), self.batch_size):
                batch_num = i // self.batch_size + 1
                chunk = symbols[i:i+self.batch_size]
                
                print(f"üîÑ Batch {batch_num}/{total_batches}: {chunk}")
                
                batch_results = []
                for j, symbol in enumerate(chunk):
                    try:
                        result = await self._update_symbol_data(session, symbol, target_end_date)
                        batch_results.append(result)
                        
                        if result.get("success"):
                            records = result.get('records', 0)
                            if records > 0:
                                self.updated_symbols.add(symbol)
                                last_date = result.get('last_updated_date', 'N/A')
                                duration = result.get('duration_seconds', 0)
                                print(f"  ‚úÖ {symbol}: {records} records to {last_date} ({duration}s)")
                            else:
                                print(f"  ‚è≠Ô∏è {symbol}: No new data")
                        else:
                            error = result.get('error', 'unknown error')
                            print(f"  ‚ùå {symbol}: {error}")
                        
                        # Delay gi·ªØa c√°c m√£
                        if j < len(chunk) - 1:
                            await asyncio.sleep(2.0)
                            
                    except Exception as e:
                        print(f"  ‚ùå {symbol}: Exception - {e}")
                        batch_results.append({"symbol": symbol, "success": False, "error": str(e)})
                
                results.extend(batch_results)
                
                # Delay gi·ªØa c√°c batch
                if i + self.batch_size < len(symbols):
                    print(f"‚è≥ Ch·ªù 5 gi√¢y tr∆∞·ªõc batch ti·∫øp theo...")
                    await asyncio.sleep(5.0)
            
            # T·ªïng k·∫øt c·∫≠p nh·∫≠t d·ªØ li·ªáu
            success_count = sum(1 for r in results if r.get("success"))
            failed_symbols = [r["symbol"] for r in results if not r.get("success")]
            total_records = sum(r.get('records', 0) for r in results if r.get("success"))
            
            return {
                "total_symbols": len(symbols),
                "success_count": success_count,
                "failed_count": len(failed_symbols),
                "failed_symbols": failed_symbols,
                "total_records": total_records,
                "target_end_date": target_end_date.isoformat()
            }
    
    async def _update_symbol_data(self, session, symbol: str, target_end_date: date) -> Dict[str, Any]:
        """C·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªôt m√£"""
        start_time = time.time()
        
        try:
            # Fetch d·ªØ li·ªáu v·ªõi tracking
            success, df, last_updated_date = await self.data_fetcher.fetch_daily_with_tracking(symbol, target_end_date)
            
            if not success:
                await self.data_fetcher.update_tracking_error(symbol, "Failed to fetch data")
                return {"symbol": symbol, "success": False, "error": "fetch_failed"}
            
            if df is None or df.empty:
                # Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi
                await self.data_fetcher.update_tracking_success(symbol, target_end_date, 0, int(time.time() - start_time))
                return {"symbol": symbol, "success": True, "records": 0, "message": "no_new_data"}
            
            # ƒê·∫£m b·∫£o stock t·ªìn t·∫°i
            stock_repo = RepositoryFactory.create_stock_repository(session)
            existing = await stock_repo.get_by_symbol(symbol)
            if not existing:
                await stock_repo.create({
                    "symbol": symbol.upper(),
                    "name": f"Stock {symbol.upper()}",
                    "exchange": "HOSE",
                    "sector": "Other",
                    "industry": "Other",
                    "market_cap_tier": "Tier 3",
                    "is_active": True,
                })
                existing = await stock_repo.get_by_symbol(symbol)
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu cho upsert
            price_repo = RepositoryFactory.create_stock_price_repository(session)
            foreign_repo = RepositoryFactory.create_foreign_trade_repository(session)
            
            prices: List[Dict[str, Any]] = []
            trades: List[Dict[str, Any]] = []
            
            for _, row in df.iterrows():
                close_price = float(row.get("close", 0.0))
                volume = int(row.get("volume", 0))
                
                prices.append({
                    "stock_id": existing.id,
                    "symbol": symbol.upper(),
                    "time": row["time"],
                    "open": float(row.get("open", 0)),
                    "high": float(row.get("high", 0)),
                    "low": float(row.get("low", 0)),
                    "close": close_price,
                    "volume": volume,
                    "value": close_price * volume,
                    "source": DataSource.SSI,
                })
                
                fb = int(row.get("foreign_buy_shares", 0))
                fs = int(row.get("foreign_sell_shares", 0))
                
                if fb > 0 or fs > 0:
                    trades.append({
                        "stock_id": existing.id,
                        "symbol": symbol.upper(),
                        "time": row["time"],
                        "buy_volume": fb,
                        "sell_volume": fs,
                        "net_volume": fb - fs,
                        "buy_value": close_price * fb,
                        "sell_value": close_price * fs,
                        "net_value": close_price * (fb - fs),
                        "source": DataSource.SSI,
                    })
            
            # Upsert d·ªØ li·ªáu
            if prices:
                await price_repo.create_batch(prices)
            if trades:
                await foreign_repo.create_batch(trades)
            
            # C·∫≠p nh·∫≠t tracking
            duration_seconds = int(time.time() - start_time)
            await self.data_fetcher.update_tracking_success(symbol, last_updated_date, len(df), duration_seconds)
            
            return {
                "symbol": symbol, 
                "success": True, 
                "records": len(df),
                "last_updated_date": last_updated_date.isoformat(),
                "duration_seconds": duration_seconds
            }
            
        except Exception as e:
            error_msg = str(e)
            await self.data_fetcher.update_tracking_error(symbol, error_msg)
            return {"symbol": symbol, "success": False, "error": error_msg}
    
    async def _run_analysis_for_updated_symbols(self) -> Dict[str, Any]:
        """Ch·∫°y ph√¢n t√≠ch cho c√°c m√£ ƒë√£ c·∫≠p nh·∫≠t"""
        if not self.updated_symbols:
            print("‚è≠Ô∏è Kh√¥ng c√≥ m√£ n√†o ƒë∆∞·ª£c c·∫≠p nh·∫≠t, b·ªè qua ph√¢n t√≠ch")
            return {
                "total_symbols": 0,
                "success_count": 0,
                "failed_count": 0,
                "failed_symbols": [],
                "total_signals": 0
            }
        
        print(f"üìà Ph√¢n t√≠ch {len(self.updated_symbols)} m√£ ƒë√£ c·∫≠p nh·∫≠t")
        
        # T√≠nh to√°n th·ªùi gian ph√¢n t√≠ch
        end_date = date.today()
        start_date = end_date - timedelta(days=self.analysis_days)
        
        print(f"üìÖ Th·ªùi gian ph√¢n t√≠ch: {start_date} ƒë·∫øn {end_date}")
        
        results: List[Dict[str, Any]] = []
        total_signals = 0
        
        for symbol in self.updated_symbols:
            try:
                print(f"üîç Ph√¢n t√≠ch {symbol}...")
                
                # Ch·∫°y ph√¢n t√≠ch
                result = await self.analysis_engine.analyze_symbol(
                    symbol=symbol,
                    start_date=start_date.isoformat(),
                    end_date=end_date.isoformat()
                )
                
                signals_count = len(result.signals)
                total_signals += signals_count
                self.analyzed_symbols.add(symbol)
                
                results.append({
                    "symbol": symbol,
                    "success": True,
                    "signals_count": signals_count,
                    "analysis_result_id": result.analysis_result_id
                })
                
                print(f"  ‚úÖ {symbol}: {signals_count} t√≠n hi·ªáu")
                
                # Delay nh·ªè gi·ªØa c√°c ph√¢n t√≠ch
                await asyncio.sleep(1.0)
                
            except Exception as e:
                print(f"  ‚ùå {symbol}: {e}")
                results.append({
                    "symbol": symbol,
                    "success": False,
                    "error": str(e)
                })
        
        # T·ªïng k·∫øt ph√¢n t√≠ch
        success_count = sum(1 for r in results if r.get("success"))
        failed_symbols = [r["symbol"] for r in results if not r.get("success")]
        
        return {
            "total_symbols": len(self.updated_symbols),
            "success_count": success_count,
            "failed_count": len(failed_symbols),
            "failed_symbols": failed_symbols,
            "total_signals": total_signals,
            "analysis_period": {
                "start_date": start_date.isoformat(),
                "end_date": end_date.isoformat(),
                "days": self.analysis_days
            }
        }


async def main(batch_size: int = 3, analysis_days: int = 30) -> None:
    """H√†m ch√≠nh"""
    pipeline = VN100AnalysisPipeline(batch_size=batch_size, analysis_days=analysis_days)
    
    try:
        result = await pipeline.run_pipeline()
        
        # L∆∞u k·∫øt qu·∫£
        import json
        from datetime import datetime
        
        output_file = f"output/pipeline_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nüíæ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_file}")
        
    except Exception as e:
        print(f"‚ùå Pipeline th·∫•t b·∫°i: {e}")
        import traceback
        traceback.print_exc()
        raise


def cli_main() -> int:
    """CLI entry point"""
    parser = argparse.ArgumentParser(description="VN100 Analysis Pipeline")
    parser.add_argument("--batch", dest="batch", type=int, default=3, 
                       help="Batch size for data update (default: 3)")
    parser.add_argument("--analysis-days", dest="analysis_days", type=int, default=30,
                       help="Number of days for analysis (default: 30)")
    
    args = parser.parse_args()
    
    asyncio.run(main(args.batch, args.analysis_days))
    return 0


if __name__ == "__main__":
    raise SystemExit(cli_main())
